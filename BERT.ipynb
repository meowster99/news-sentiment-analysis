{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "# import tensorflow_text as text\n",
    "# from official.nlp import optimization  # to create AdamW optimizer\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint,LearningRateScheduler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "import random \n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e8be68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the train and test files for financial news\n",
    "fnews_Xtrain = pd.read_csv('data/train/fnews_Xtrain.csv')\n",
    "fnews_Xtest = pd.read_csv('data/test/fnews_Xtest.csv')\n",
    "fnews_ytrain = pd.read_csv('data/train/fnews_ytrain.csv')\n",
    "fnews_ytest = pd.read_csv('data/test/fnews_ytest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3876, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnews_Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(970, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnews_Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0a9030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the first column for all the train and test sets\n",
    "fnews_Xtrain.drop(columns=fnews_Xtrain.columns[0], axis=1, inplace=True)\n",
    "fnews_ytrain.drop(columns=fnews_ytrain.columns[0], axis=1, inplace=True)\n",
    "fnews_Xtest.drop(columns=fnews_Xtest.columns[0], axis=1, inplace=True)\n",
    "fnews_ytest.drop(columns=fnews_ytest.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edb8465c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (3876, 1) ytrain: (3876, 1)\n",
      "Xtest: (970, 1) ytest: (970, 1)\n"
     ]
    }
   ],
   "source": [
    "#check the respective shape of the train and test sets\n",
    "print('Xtrain:',fnews_Xtrain.shape, 'ytrain:' ,fnews_ytrain.shape)\n",
    "print('Xtest:',fnews_Xtest.shape, 'ytest:' ,fnews_ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dec96a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['russia', 'raisio', 's', 'food', 'division', 's', 'home', 'market', 'stretch', 'way', 'vladivostok']\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnews_Xtrain['News'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1078dd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fnews_Xtrain[\"News\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "021c6933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['russia', 'raisio', 's', 'food', 'division', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['operator', 'need', 'learn', 'use', 'device',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['company', 'expects', 'net', 'sale', 'half', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['bridge', 'km', 'long', 'located', 'anasmotet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['nokia', 'capcom', 'announced', 'resident', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News\n",
       "0  ['russia', 'raisio', 's', 'food', 'division', ...\n",
       "1  ['operator', 'need', 'learn', 'use', 'device',...\n",
       "2  ['company', 'expects', 'net', 'sale', 'half', ...\n",
       "3  ['bridge', 'km', 'long', 'located', 'anasmotet...\n",
       "4  ['nokia', 'capcom', 'announced', 'resident', '..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnews_Xtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnews_Xtrain['News'] = fnews_Xtrain['News'].apply(eval)\n",
    "fnews_Xtest['News'] = fnews_Xtest['News'].apply(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is a trained Transformer Encoder stack. We will be using both the base and the large version from huggingface, and choose the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT requires specifically formatted inputs. For each tokenized input sentence, we need to create:\n",
    "- input ids: a sequence of integers identifying each input token to its index number in the BERT tokenizer vocabulary\n",
    "- segment mask: (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence\n",
    "- attention mask: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens (we'll detail this in the next paragraph)\n",
    "- labels: a single value of 1 or 0. In our task 1 means \"grammatical\" and 0 means \"ungrammatical\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can have variable length input sentences, BERT does requires our input arrays to be the same size. We address this by first choosing a maximum sentence length, and then padding and truncating our inputs until every input sequence is of the same length.\n",
    "\n",
    "To \"pad\" our inputs in this context means that if a sentence is shorter than the maximum sentence length, we simply add 0s to the end of the sequence until it is the maximum sentence length.\n",
    "\n",
    "If a sentence is longer than the maximum sentence length, then we simply truncate the end of the sequence, discarding anything that does not fit into our maximum sentence length.\n",
    "\n",
    "We pad and truncate our sequences so that they all become of length MAX_LEN (\"post\" indicates that we want to pad and truncate at the end of the sequence, as opposed to the beginning) pad_sequences is a utility function that we're borrowing from Keras. It simply handles the truncating and padding of Python lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, TFBertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "def create_attention_masks(df):\n",
    "    # We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "    sentences = [[\"[CLS]\"] + sentence + [\"[SEP]\"] for sentence in df]\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "    tokenized_texts = [tokenizer.tokenize(\" \".join(sent)) for sent in sentences] # retokensized it according to BERT vocabulary\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    # Create attention masks\n",
    "    attention_masks = []\n",
    "\n",
    "    # Create a mask of 1s for each token followed by 0s for padding\n",
    "    for seq in input_ids:\n",
    "        train_seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(train_seq_mask)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the classes to a numeric representation\n",
    "train_labels = fnews_ytrain['Class'].map({'neutral':0, 'positive': 1, 'negative': -1}).values\n",
    "test_labels = fnews_ytest['Class'].map({'neutral':0, 'positive': 1, 'negative': -1}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  3100 3100 3100\n",
      "Val shape:  776 776 776\n",
      "Test shape:  970 970 970\n"
     ]
    }
   ],
   "source": [
    "train_input_ids, train_attention_masks = create_attention_masks(fnews_Xtrain['News'])\n",
    "test_input_ids, test_attention_masks = create_attention_masks(fnews_Xtest['News'])\n",
    "ttrain_input_ids, val_input_ids, ttrain_attention_masks, val_attention_masks, ttrain_labels, val_labels  = train_test_split(train_input_ids, train_attention_masks, train_labels, test_size=0.2)\n",
    "print('Train shape: ', len(ttrain_input_ids), len(ttrain_attention_masks), len(ttrain_labels))\n",
    "print('Val shape: ', len(val_input_ids), len(val_attention_masks), len(val_labels))\n",
    "print('Test shape: ', len(test_input_ids), len(test_attention_masks), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttrain_inputs = tf.convert_to_tensor(train_input_ids, dtype=tf.float64)\n",
    "val_inputs = tf.convert_to_tensor(val_input_ids, dtype=tf.float64)\n",
    "test_inputs = tf.convert_to_tensor(test_input_ids, dtype=tf.float64)\n",
    "ttrain_labels = tf.convert_to_tensor(ttrain_labels, dtype=tf.float64)\n",
    "val_labels = tf.convert_to_tensor(val_labels, dtype=tf.float64)\n",
    "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.float64)\n",
    "ttrain_masks = tf.convert_to_tensor(ttrain_attention_masks, dtype=tf.float64)\n",
    "val_masks = tf.convert_to_tensor(val_attention_masks, dtype=tf.float64)\n",
    "test_masks = tf.convert_to_tensor(test_attention_masks, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model: BERT with 1 layer unfrozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_ids = tf.keras.layers.Input((3100,), dtype=tf.float64)\n",
    "    attention_masks = tf.keras.layers.Input((3100,), dtype=tf.float64)\n",
    "\n",
    "    config = BertConfig() \n",
    "    config.output_hidden_states = False \n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-uncased',\n",
    "                                             config=config)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()([input_ids, attention_masks])\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_masks], \n",
    "                                  outputs=x)\n",
    "    return model, bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "ttrain_inputs_scaled = scaler.fit_transform(ttrain_inputs)\n",
    "val_inputs_scaled = scaler.transform(val_inputs)\n",
    "test_inputs_scaled = scaler.transform(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "batch_size = 8\n",
    "patients = 3\n",
    "learning_rate = 3e-5\n",
    "n_validate_per_epoch = 5\n",
    "model, _ = create_model()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "init_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 3876\n  y sizes: 3100\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexy\\OneDrive\\Documents\\GitHub\\news-sentiment-analysis\\BERT.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexy/OneDrive/Documents/GitHub/news-sentiment-analysis/BERT.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_preds \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexy/OneDrive/Documents/GitHub/news-sentiment-analysis/BERT.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mset_weights(init_weights)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alexy/OneDrive/Documents/GitHub/news-sentiment-analysis/BERT.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(ttrain_inputs_scaled, ttrain_labels,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexy/OneDrive/Documents/GitHub/news-sentiment-analysis/BERT.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                 validation_data\u001b[39m=\u001b[39;49m(val_inputs_scaled, val_labels),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexy/OneDrive/Documents/GitHub/news-sentiment-analysis/BERT.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                 epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexy/OneDrive/Documents/GitHub/news-sentiment-analysis/BERT.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                 batch_size\u001b[39m=\u001b[39;49mbatch_size,)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexy/OneDrive/Documents/GitHub/news-sentiment-analysis/BERT.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m t_preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(test_inputs, batch_size\u001b[39m=\u001b[39mbatch_size)\n",
      "File \u001b[1;32mc:\\Users\\alexy\\Environments\\y4-env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\alexy\\Environments\\y4-env\\lib\\site-packages\\keras\\engine\\data_adapter.py:1851\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1845\u001b[0m         label,\n\u001b[0;32m   1846\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m   1847\u001b[0m             \u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1848\u001b[0m         ),\n\u001b[0;32m   1849\u001b[0m     )\n\u001b[0;32m   1850\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1851\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 3876\n  y sizes: 3100\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "model.set_weights(init_weights)\n",
    "model.fit(ttrain_inputs_scaled, ttrain_labels,\n",
    "                validation_data=(val_inputs_scaled, val_labels),\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,)\n",
    "        \n",
    "t_preds = model.predict(test_inputs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('y4-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f3c75af1d3d8bb12916f6b5cd792222eec44742c15047e8000f37733dd2d0a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
